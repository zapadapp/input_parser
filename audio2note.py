import os
import librosa
import numpy as np
import librosa.display
from scipy.signal import find_peaks
from scipy.fft import fft
from music21 import note
from music21 import stream as m21stream


## Function that filters lower samples generated by input noise.
def filterLowSamples(samples):
    # find indexes of all elements lower than 2000 from samples
    indexes = np.where(samples < 2000)

    # remove elements for given indexes
    return np.delete(samples, indexes)

def detectAndPrintNote(s,q, y, sr, samples, a, b, scorePath):
    # limit the audio input from sample in index a to sample in index b, unless b is 999 which means that it is the end of the audio data
    if b == 999:
        data = y[samples[a]:]
    else:    
        data = y[samples[a]:samples[b]]
    
    # calculate FFT and absolute values
    X = fft(data)
    X_mag = np.absolute(X)

    # generate x values (frequencies)
    f = np.linspace(0, sr, len(X_mag))
    f_bins = int(len(X_mag)*0.1) 
    
    #plt.plot(f[:f_bins], X_mag[:f_bins])

    # find peaks in Y values. Use height value to filter lower peaks
    _, properties = find_peaks(X_mag, height=100)
    if len(properties['peak_heights']) > 0:
        y_peak = properties['peak_heights'][0]

        # get index for the peak
        peak_i = np.where(X_mag[:f_bins] == y_peak)

        # if we found an for a peak we print the note
        if len(peak_i) > 0:
            nota = convertToNote(str(librosa.hz_to_note(f[peak_i[0]])))
            print("Detected note: {}".format(nota))
            q.put(nota)
            s.append(note.Note(nota))
            print(s.write('lily.png', fp=os.path.join("tmp", scorePath)))
            #q.put(s)
            #print("Detected note: {}".format(str(librosa.hz_to_note(f[peak_i[0]]))))
            return 

    #print("No detected note")    

def convertToNote(val) :
    nota = str.replace(str.replace(val, "['", ""), "']", "")
    if len(nota) == 3 :
        nota = nota[0] + "#" + nota[2]

    return nota

def consumer(in_q):
    while True:
        # Get some data
        s = in_q.get()
        print(s.write('lily.png', fp='tmp/score'))

def processAudio(s,q, audioPath, scorePath):
    ############################################################
    ##############    Actual audio processing    ###############
    ############################################################

    # Note: we should be doing this in another thread so that we do not have to wait for this to finish to record another wave file.
    
    y, sr = librosa.load(audioPath)
    # generate image showing the audio input with the onset times
    #plt.figure(figsize=(14, 5))
    #librosa.display.waveshow(y, sr)

    onset_frames = librosa.onset.onset_detect(y, sr=sr, wait=1, pre_avg=1, post_avg=1, pre_max=1, post_max=1)
    #onset_times = librosa.frames_to_time(onset_frames)
    
    #plt.vlines(onset_times, -0.8, 0.79, color='r', alpha=0.8) 
    #plt.savefig(image_path)
    # plt.show()
    # plt.close()

    # convert frames to samples
    samples = librosa.frames_to_samples(onset_frames)

    # filter lower samples
    filteredSamples = filterLowSamples(samples)

    # get indexes of all samples in the numpy array
    indexes = np.where(filteredSamples>0)

    length = len(indexes[0])
    j = 0

    # iterate over all indexes of the onsets. What we do here is group indexes by two, so that we have the beginning and ending of 
    # the audio sample that we will use to get the note from.
    # For example, if we have 4 onsets (indexes 0 to 3) we use these segments:
    # 0 to 1
    # 1 to 2
    # 2 to 3
    # Next step: we should also use whatever piece of audio before the first index and after the last one.
    for i in indexes[0]:
        j = i
        
        if j < length-1:
            detectAndPrintNote(s, q, y, sr, filteredSamples, j, j+1, scorePath)
        elif j == length-1:
            detectAndPrintNote(s, q, y, sr, filteredSamples, j, 999, scorePath)
